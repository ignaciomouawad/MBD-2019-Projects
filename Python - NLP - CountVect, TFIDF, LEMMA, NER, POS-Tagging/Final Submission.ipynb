{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Assignment 1 - Fake or Real News\n",
    "\n",
    "## Natural Language Processing and Text Mining\n",
    "\n",
    "By: Moutaz Al-Huneidi and Ignacio Mouawad\n",
    "\n",
    "IE MBD 2019 A1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________________________________________\n",
    "\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from matplotlib import pyplot\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import untag\n",
    "from nltk import UnigramTagger\n",
    "from nltk import RegexpTagger\n",
    "from nltk import AffixTagger\n",
    "from nltk import NgramTagger\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________________________________________________________\n",
    "\n",
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = \"C:/Users/ignac/Desktop/ie/mbd term 3/NLP/assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_train_csv = os.path.join(DATA_DIRECTORY, \"fake_or_real_news_training.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_test_csv = os.path.join(DATA_DIRECTORY, \"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label   X1   X2  \n",
       "ID                                                                        \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  NaN  NaN  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  NaN  NaN  \n",
       "875    It's primary day in New York and front-runners...  REAL  NaN  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_train_original = pd.read_csv(\n",
    "    news_train_csv,\n",
    "    index_col=\"ID\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "news_train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "2439   Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "864    Sanders, Cruz resist pressure after NY losses,...   \n",
       "4128   Surviving escaped prisoner likely fatigued and...   \n",
       "662    Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                    text  \n",
       "ID                                                        \n",
       "10498  September New Homes Sales Rise Back To 1992 Le...  \n",
       "2439   But when Congress debated and passed the Patie...  \n",
       "864    The Bernie Sanders and Ted Cruz campaigns vowe...  \n",
       "4128   Police searching for the second of two escaped...  \n",
       "662    No matter who wins California's 475 delegates ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_test = pd.read_csv(\n",
    "    news_test_csv,\n",
    "    index_col=\"ID\",\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "news_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = news_train_original.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3999</td>\n",
       "      <td>3999</td>\n",
       "      <td>3999</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3968</td>\n",
       "      <td>3839</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>OnPolitics | 's politics blog</td>\n",
       "      <td>Killing Obama administration rules, dismantlin...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>REAL</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "      <td>1990</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                title  \\\n",
       "count                            3999   \n",
       "unique                           3968   \n",
       "top     OnPolitics | 's politics blog   \n",
       "freq                                4   \n",
       "\n",
       "                                                     text label    X1    X2  \n",
       "count                                                3999  3999    33     2  \n",
       "unique                                               3839    35     4     2  \n",
       "top     Killing Obama administration rules, dismantlin...  REAL  REAL  FAKE  \n",
       "freq                                                   41  1990    17     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3999 entries, 8476 to 9673\n",
      "Data columns (total 5 columns):\n",
      "title    3999 non-null object\n",
      "text     3999 non-null object\n",
      "label    3999 non-null object\n",
      "X1       33 non-null object\n",
      "X2       2 non-null object\n",
      "dtypes: object(5)\n",
      "memory usage: 187.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title       0\n",
       "text        0\n",
       "label       0\n",
       "X1       3966\n",
       "X2       3997\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can note that there are no missing values in the title, text, or label columns in the train dataset, but there seems to be a large number of nulls in the X1 and X2 columns, so this will require further investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>Election Day: No Legal Pot In Ohio</td>\n",
       "      <td>Democrats Lose In The South</td>\n",
       "      <td>Election Day: No Legal Pot In Ohio; Democrats ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10194</th>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>Leonardo DiCaprio to the rescue?</td>\n",
       "      <td>Who rode it best? Jesse Jackson mounts up to f...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>Black Hawk crashes off Florida</td>\n",
       "      <td>human remains found</td>\n",
       "      <td>(CNN) Thick fog forced authorities to suspend ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2786</th>\n",
       "      <td>Afghanistan: 19 die in air attacks on hospital</td>\n",
       "      <td>U.S. investigating</td>\n",
       "      <td>(CNN) Aerial bombardments blew apart a Doctors...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3622</th>\n",
       "      <td>Al Qaeda rep says group directed Paris magazin...</td>\n",
       "      <td>US issues travel warning</td>\n",
       "      <td>A member of Al Qaeda's branch in Yemen said Fr...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7375</th>\n",
       "      <td>Shallow 5.4 magnitude earthquake rattles centr...</td>\n",
       "      <td>shakes buildings in Rome</td>\n",
       "      <td>00 UTC © USGS Map of the earthquake's epicent...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9097</th>\n",
       "      <td>ICE Agent Commits Suicide in NYC</td>\n",
       "      <td>Leaves Note Revealing Gov’t Plans to Round-up...</td>\n",
       "      <td>Email Print After writing a lengthy suicide no...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9203</th>\n",
       "      <td>Political Correctness for Yuengling Brewery</td>\n",
       "      <td>What About Our Opioid Epidemic?</td>\n",
       "      <td>We Are Change \\n\\nIn today’s political climate...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1602</th>\n",
       "      <td>Poll gives Biden edge over Clinton against GOP...</td>\n",
       "      <td>VP meets with Trumka</td>\n",
       "      <td>A new national poll shows Vice President Biden...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4562</th>\n",
       "      <td>Russia begins airstrikes in Syria</td>\n",
       "      <td>U.S. warns of new concerns in conflict</td>\n",
       "      <td>Russian warplanes began airstrikes in Syria on...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>Trump &amp;amp</td>\n",
       "      <td>Clinton Were Very Convincing...on How Lousy t...</td>\n",
       "      <td>Let's pretend for a moment that the biggest he...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3508</th>\n",
       "      <td>Belgian police mount raids</td>\n",
       "      <td>prosecutors acknowledge missed opportunities</td>\n",
       "      <td>Belgian authorities missed a chance to press a...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7559</th>\n",
       "      <td>STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...</td>\n",
       "      <td>GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS</td>\n",
       "      <td>Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3634</th>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues</td>\n",
       "      <td>Brothers Were On No-Fly List</td>\n",
       "      <td>The Latest On Paris Attack: Manhunt Continues;...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8470</th>\n",
       "      <td>The Amish In America Commit Their Vote To Dona...</td>\n",
       "      <td>Mathematically Guaranteeing Him A Presidentia...</td>\n",
       "      <td>18 SHARE The Amish in America have committed t...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6404</th>\n",
       "      <td>#BREAKING: SECOND Assassination Attempt On Tru...</td>\n",
       "      <td>Suspect Detained (LIVE BLOG)</td>\n",
       "      <td>We Are Change \\nDonald Trump on Saturday was q...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>30th Infantry Division: “Work Horse of the Wes...</td>\n",
       "      <td>The Big Picture TV-211</td>\n",
       "      <td>Published on Oct 27, 2016 by Jeff Quitney The ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Planned Parenthood’s lobbying effort</td>\n",
       "      <td>pay raises for federal workers</td>\n",
       "      <td>and the future Fed rates</td>\n",
       "      <td>PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10492</th>\n",
       "      <td>TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...</td>\n",
       "      <td>“THE END OF LIFE AS WE KNOW IT”</td>\n",
       "      <td>Paul Joseph Watson Senior British army officer...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>Inside The Mind Of An FBI Informant</td>\n",
       "      <td>Terri Linnell Admits Role As Gov’t Snitch</td>\n",
       "      <td>Inside The Mind Of An FBI Informant; Terri Lin...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4953</th>\n",
       "      <td>Gary Johnson Avoids Typical Third-Party Fade</td>\n",
       "      <td>Best Polling Since Perot in ‘92</td>\n",
       "      <td>A couple of weeks ago in this space I pushed b...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Nearly 300K New Jobs In February</td>\n",
       "      <td>Unemployment Dips To 5.5 Percent</td>\n",
       "      <td>Nearly 300K New Jobs In February; Unemployment...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5741</th>\n",
       "      <td>Why Trump Won</td>\n",
       "      <td>Why Clinton Lost</td>\n",
       "      <td>WashingtonsBlog \\nBy Robert Parry, the inves...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4131</th>\n",
       "      <td>Jesse Matthew charged in Hannah Graham's murder</td>\n",
       "      <td>DA will not pursue death penalty</td>\n",
       "      <td>Jesse Matthew Jr., a former hospital worker, w...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8748</th>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRiot</td>\n",
       "      <td>Media Ignores (Video)</td>\n",
       "      <td>WATCH: Mass Shooting Occurs During #TrumpRio...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6717</th>\n",
       "      <td>Jim Rogers: It’s Time To Prepare</td>\n",
       "      <td>Economic And Financial Collapse Imminent (VIDEO)</td>\n",
       "      <td>By: The Voice of Reason | Regardless of how mu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943</th>\n",
       "      <td>Islamic State admits defeat in Kobani</td>\n",
       "      <td>blames airstrikes</td>\n",
       "      <td>Islamic State militants have acknowledged for ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5248</th>\n",
       "      <td>Clinton Cries Racism Tagging Trump with KKK</td>\n",
       "      <td>Trump Says 'She Lies'</td>\n",
       "      <td>With only about 70 days left until the electio...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3624</th>\n",
       "      <td>Suspects In Paris Magazine Attack Killed</td>\n",
       "      <td>Market Gunman And 4 Hostages Also Dead</td>\n",
       "      <td>Suspects In Paris Magazine Attack Killed; Mark...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6268</th>\n",
       "      <td>Chart Of The Day: Since 2009—–Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>Chart Of The Day: Since 2009 Recovery For The 5%</td>\n",
       "      <td>Stagnation for the 95%</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2738</th>\n",
       "      <td>Ted Cruz launches bid</td>\n",
       "      <td>Some pundits paint him as scary extremist</td>\n",
       "      <td>Before he got to repealing ObamaCare, before h...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>State Dept. IDs 2 Americans killed in Nepal quake</td>\n",
       "      <td>2 others reportedly dead</td>\n",
       "      <td>The State Department identified two Americans ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9954</th>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>bursting of firecrackers suspected</td>\n",
       "      <td>Incredible smoke haze seen outside NDTV office...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "599                   Election Day: No Legal Pot In Ohio   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356                       Black Hawk crashes off Florida   \n",
       "2786      Afghanistan: 19 die in air attacks on hospital   \n",
       "3622   Al Qaeda rep says group directed Paris magazin...   \n",
       "7375   Shallow 5.4 magnitude earthquake rattles centr...   \n",
       "9097                    ICE Agent Commits Suicide in NYC   \n",
       "9203         Political Correctness for Yuengling Brewery   \n",
       "1602   Poll gives Biden edge over Clinton against GOP...   \n",
       "4562                   Russia begins airstrikes in Syria   \n",
       "4748                                          Trump &amp   \n",
       "3508                          Belgian police mount raids   \n",
       "7559   STATE OF GEORGIA FIRES PASTOR BECAUSE OF HIS F...   \n",
       "3634       The Latest On Paris Attack: Manhunt Continues   \n",
       "8470   The Amish In America Commit Their Vote To Dona...   \n",
       "6404   #BREAKING: SECOND Assassination Attempt On Tru...   \n",
       "10499  30th Infantry Division: “Work Horse of the Wes...   \n",
       "9                   Planned Parenthood’s lobbying effort   \n",
       "10492  TOP BRITISH GENERAL WARNS OF NUCLEAR WAR WITH ...   \n",
       "10138                Inside The Mind Of An FBI Informant   \n",
       "4953        Gary Johnson Avoids Typical Third-Party Fade   \n",
       "496                     Nearly 300K New Jobs In February   \n",
       "5741                                       Why Trump Won   \n",
       "4131     Jesse Matthew charged in Hannah Graham's murder   \n",
       "8748       WATCH: Mass Shooting Occurs During #TrumpRiot   \n",
       "6717                    Jim Rogers: It’s Time To Prepare   \n",
       "2943               Islamic State admits defeat in Kobani   \n",
       "5248         Clinton Cries Racism Tagging Trump with KKK   \n",
       "3624            Suspects In Paris Magazine Attack Killed   \n",
       "6268   Chart Of The Day: Since 2009—–Recovery For The 5%   \n",
       "2738                               Ted Cruz launches bid   \n",
       "4025   State Dept. IDs 2 Americans killed in Nepal quake   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "599                          Democrats Lose In The South   \n",
       "10194                   Leonardo DiCaprio to the rescue?   \n",
       "356                                  human remains found   \n",
       "2786                                  U.S. investigating   \n",
       "3622                            US issues travel warning   \n",
       "7375                            shakes buildings in Rome   \n",
       "9097    Leaves Note Revealing Gov’t Plans to Round-up...   \n",
       "9203                     What About Our Opioid Epidemic?   \n",
       "1602                                VP meets with Trumka   \n",
       "4562              U.S. warns of new concerns in conflict   \n",
       "4748    Clinton Were Very Convincing...on How Lousy t...   \n",
       "3508        prosecutors acknowledge missed opportunities   \n",
       "7559        GOVERNMENT DIDN’T “APPROVE” BIBLICAL SERMONS   \n",
       "3634                        Brothers Were On No-Fly List   \n",
       "8470    Mathematically Guaranteeing Him A Presidentia...   \n",
       "6404                        Suspect Detained (LIVE BLOG)   \n",
       "10499                             The Big Picture TV-211   \n",
       "9                         pay raises for federal workers   \n",
       "10492                    “THE END OF LIFE AS WE KNOW IT”   \n",
       "10138          Terri Linnell Admits Role As Gov’t Snitch   \n",
       "4953                     Best Polling Since Perot in ‘92   \n",
       "496                     Unemployment Dips To 5.5 Percent   \n",
       "5741                                    Why Clinton Lost   \n",
       "4131                    DA will not pursue death penalty   \n",
       "8748                               Media Ignores (Video)   \n",
       "6717    Economic And Financial Collapse Imminent (VIDEO)   \n",
       "2943                                   blames airstrikes   \n",
       "5248                               Trump Says 'She Lies'   \n",
       "3624              Market Gunman And 4 Hostages Also Dead   \n",
       "6268                              Stagnation for the 95%   \n",
       "2738           Some pundits paint him as scary extremist   \n",
       "4025                            2 others reportedly dead   \n",
       "9954                  bursting of firecrackers suspected   \n",
       "\n",
       "                                                   label  \\\n",
       "ID                                                         \n",
       "599    Election Day: No Legal Pot In Ohio; Democrats ...   \n",
       "10194  Who rode it best? Jesse Jackson mounts up to f...   \n",
       "356    (CNN) Thick fog forced authorities to suspend ...   \n",
       "2786   (CNN) Aerial bombardments blew apart a Doctors...   \n",
       "3622   A member of Al Qaeda's branch in Yemen said Fr...   \n",
       "7375    00 UTC © USGS Map of the earthquake's epicent...   \n",
       "9097   Email Print After writing a lengthy suicide no...   \n",
       "9203   We Are Change \\n\\nIn today’s political climate...   \n",
       "1602   A new national poll shows Vice President Biden...   \n",
       "4562   Russian warplanes began airstrikes in Syria on...   \n",
       "4748   Let's pretend for a moment that the biggest he...   \n",
       "3508   Belgian authorities missed a chance to press a...   \n",
       "7559   Home › SOCIETY | US NEWS › STATE OF GEORGIA FI...   \n",
       "3634   The Latest On Paris Attack: Manhunt Continues;...   \n",
       "8470   18 SHARE The Amish in America have committed t...   \n",
       "6404   We Are Change \\nDonald Trump on Saturday was q...   \n",
       "10499  Published on Oct 27, 2016 by Jeff Quitney The ...   \n",
       "9                               and the future Fed rates   \n",
       "10492  Paul Joseph Watson Senior British army officer...   \n",
       "10138  Inside The Mind Of An FBI Informant; Terri Lin...   \n",
       "4953   A couple of weeks ago in this space I pushed b...   \n",
       "496    Nearly 300K New Jobs In February; Unemployment...   \n",
       "5741     WashingtonsBlog \\nBy Robert Parry, the inves...   \n",
       "4131   Jesse Matthew Jr., a former hospital worker, w...   \n",
       "8748     WATCH: Mass Shooting Occurs During #TrumpRio...   \n",
       "6717   By: The Voice of Reason | Regardless of how mu...   \n",
       "2943   Islamic State militants have acknowledged for ...   \n",
       "5248   With only about 70 days left until the electio...   \n",
       "3624   Suspects In Paris Magazine Attack Killed; Mark...   \n",
       "6268    Chart Of The Day: Since 2009 Recovery For The 5%   \n",
       "2738   Before he got to repealing ObamaCare, before h...   \n",
       "4025   The State Department identified two Americans ...   \n",
       "9954   Incredible smoke haze seen outside NDTV office...   \n",
       "\n",
       "                                                      X1    X2  \n",
       "ID                                                              \n",
       "599                                                 REAL   NaN  \n",
       "10194                                               FAKE   NaN  \n",
       "356                                                 REAL   NaN  \n",
       "2786                                                REAL   NaN  \n",
       "3622                                                REAL   NaN  \n",
       "7375                                                FAKE   NaN  \n",
       "9097                                                FAKE   NaN  \n",
       "9203                                                FAKE   NaN  \n",
       "1602                                                REAL   NaN  \n",
       "4562                                                REAL   NaN  \n",
       "4748                                                REAL   NaN  \n",
       "3508                                                REAL   NaN  \n",
       "7559                                                FAKE   NaN  \n",
       "3634                                                REAL   NaN  \n",
       "8470                                                FAKE   NaN  \n",
       "6404                                                FAKE   NaN  \n",
       "10499                                               FAKE   NaN  \n",
       "9      PLANNED PARENTHOOD’S LOBBYING GETS AGGRESSIVE....  REAL  \n",
       "10492                                               FAKE   NaN  \n",
       "10138                                               FAKE   NaN  \n",
       "4953                                                REAL   NaN  \n",
       "496                                                 REAL   NaN  \n",
       "5741                                                FAKE   NaN  \n",
       "4131                                                REAL   NaN  \n",
       "8748                                                FAKE   NaN  \n",
       "6717                                                FAKE   NaN  \n",
       "2943                                                REAL   NaN  \n",
       "5248                                                REAL   NaN  \n",
       "3624                                                REAL   NaN  \n",
       "6268                            Stagnation for the 95%    FAKE  \n",
       "2738                                                REAL   NaN  \n",
       "4025                                                REAL   NaN  \n",
       "9954                                                FAKE   NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-train[\"X1\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can note that there appears to be a separation between the main title and the subtitle of an article, resulting in the text being shifted into the next column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 has 33 non-null values\n",
      "X2 has 2 non-null values\n"
     ]
    }
   ],
   "source": [
    "print(\"X1 has %s non-null values\" % len(train[-train[\"X1\"].isna()]))\n",
    "print(\"X2 has %s non-null values\" % len(train[-train[\"X2\"].isna()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 33 rows that have misplaced labels ovarall, of which there are two rows that have 2 subtitles shifting the label to X2, thereby leaving 31 rows with their label in the X1 column. These rows need to be ammended such that they adhere to the format of the remainder of the dataset. \n",
    "\n",
    "To achieve this, if `X2` is not null, then the value of the `title` column for that row will be made to equal the concatenation of the `title`, `text`, and `X1` columns, and the `label` will be made to equal the `X2` column. Following that, the columns `X1` and `X2` will be set to equal nulls.\n",
    "\n",
    "For the 31 rows with their labels in `X1`, the `title` column for that row will be made to equal the concatenation of the `title` and `text` columns, and the `label` will be made to equal the value in the `X1` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train)):\n",
    "    if not pd.isnull(train.iloc[i].X2):\n",
    "        train[\"title\"].iloc[i] = train[\"title\"].iloc[i] + \" \" + train[\"text\"].iloc[i] + \" \" + train[\"label\"].iloc[i]\n",
    "        train[\"text\"].iloc[i] = train[\"X1\"].iloc[i]\n",
    "        train[\"label\"].iloc[i] = train[\"X2\"].iloc[i]\n",
    "        train[\"X1\"].iloc[i] = np.nan\n",
    "        train[\"X2\"].iloc[i] = np.nan\n",
    "\n",
    "for i in range(len(train)):\n",
    "    if not pd.isnull(train.iloc[i].X1):\n",
    "        train[\"title\"].iloc[i] = train[\"title\"].iloc[i] + \" \" + train[\"text\"].iloc[i]\n",
    "        train[\"text\"].iloc[i] = train[\"label\"].iloc[i]\n",
    "        train[\"label\"].iloc[i] = train[\"X1\"].iloc[i]\n",
    "        train[\"X1\"].iloc[i] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that there are no columns remaining with misplaced tags in columns X1 and X2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, text, label, X1, X2]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-train[\"X1\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, text, label, X1, X2]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[-train[\"X2\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of those columns are now empty and can be safely dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        You Can Smell Hillary’s Fear   \n",
       "10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "3608         Kerry to go to Paris in gesture of sympathy   \n",
       "10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "875     The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                    text label  \n",
       "ID                                                              \n",
       "8476   Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "10294  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "3608   U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "10142  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "875    It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop([\"X1\", \"X2\"], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the columns have been corrected, the processes of cleaning and pre-processing the data can begin.\n",
    "\n",
    "To start, the contents of the `title` and `text` columns will be converted to lowercase letters, as this will allow us to work with the data in a more consistent manner in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].str.lower()\n",
    "train[\"title\"] = train[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_copy = train.copy()\n",
    "labels = train_copy[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data, there are symbols and numerical values within the `title` and  `text` and fields, which may influence the manner in which the data is considered in future steps. Therefore they will be removed using a regex command.\n",
    "\n",
    "To do this, firstly, a function needs to be defined that can be applied to the dataframe, in this function the only non alpha character that will be kept is the apostrophe, as it can be seen in contractions or possessive nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(text):\n",
    "    return re.sub (r\"([^a-zA-Z'’ ]+?)\", '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"title\"] = train[\"title\"].apply(regex, 0)\n",
    "train[\"text\"] = train[\"text\"].apply(regex, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>you can smell hillary’s fear</td>\n",
       "      <td>daniel greenfield a shillman journalism fellow...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>kerry to go to paris in gesture of sympathy</td>\n",
       "      <td>us secretary of state john f kerry said monday...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>kaydee king kaydeeking november   the lesson ...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>the battle of new york why this primary matters</td>\n",
       "      <td>it's primary day in new york and frontrunners ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        you can smell hillary’s fear   \n",
       "10294  watch the exact moment paul ryan committed pol...   \n",
       "3608         kerry to go to paris in gesture of sympathy   \n",
       "10142  bernie supporters on twitter erupt in anger ag...   \n",
       "875      the battle of new york why this primary matters   \n",
       "\n",
       "                                                    text label  \n",
       "ID                                                              \n",
       "8476   daniel greenfield a shillman journalism fellow...  FAKE  \n",
       "10294  google pinterest digg linkedin reddit stumbleu...  FAKE  \n",
       "3608   us secretary of state john f kerry said monday...  REAL  \n",
       "10142   kaydee king kaydeeking november   the lesson ...  FAKE  \n",
       "875    it's primary day in new york and frontrunners ...  REAL  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the models on the `title` and `text` fields separately, the results were sub-optimal and the dataframes were very large and caused memory errors in python. Based on that, the a new column was made that combined the text in both of those columns into `total_text`. \n",
    "\n",
    "NOTE: All code cells that are in comments are left over to show what was intitally done. Though, some of the cells were written over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>you can smell hillary’s fear</td>\n",
       "      <td>daniel greenfield a shillman journalism fellow...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>kerry to go to paris in gesture of sympathy</td>\n",
       "      <td>us secretary of state john f kerry said monday...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>kaydee king kaydeeking november   the lesson ...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>the battle of new york why this primary matters</td>\n",
       "      <td>it's primary day in new york and frontrunners ...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "8476                        you can smell hillary’s fear   \n",
       "10294  watch the exact moment paul ryan committed pol...   \n",
       "3608         kerry to go to paris in gesture of sympathy   \n",
       "10142  bernie supporters on twitter erupt in anger ag...   \n",
       "875      the battle of new york why this primary matters   \n",
       "\n",
       "                                                    text label  \\\n",
       "ID                                                               \n",
       "8476   daniel greenfield a shillman journalism fellow...  FAKE   \n",
       "10294  google pinterest digg linkedin reddit stumbleu...  FAKE   \n",
       "3608   us secretary of state john f kerry said monday...  REAL   \n",
       "10142   kaydee king kaydeeking november   the lesson ...  FAKE   \n",
       "875    it's primary day in new york and frontrunners ...  REAL   \n",
       "\n",
       "                                              total_text  \n",
       "ID                                                        \n",
       "8476   you can smell hillary’s fear daniel greenfield...  \n",
       "10294  watch the exact moment paul ryan committed pol...  \n",
       "3608   kerry to go to paris in gesture of sympathy us...  \n",
       "10142  bernie supporters on twitter erupt in anger ag...  \n",
       "875    the battle of new york why this primary matter...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"total_text\"] = train[\"title\"] + \" \" + train[\"text\"]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `title` and `text` columns were then dropped, as all of their information is contained inside of `total_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text\n",
       "ID                                                            \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...\n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...\n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...\n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...\n",
       "875    REAL  the battle of new york why this primary matter..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.drop([\"title\", \"text\"], axis=1, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with the processing of the data, TF-IDF analysis will be run on both the `title` and the `text` columns of the training data.\n",
    "\n",
    "NOTE: This was changed and only applied on the `total_text` column, as explained above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving forward, the words in each of the articles need to be tokenized, so that they can be tagged for their part of speech. Given that we are trying to classify the entire document, the tokenization will be conducted on the entire document, rather than breaking down the document into sentences and then applying the tokenization on each sentence.\n",
    "\n",
    "To start, a new dataframe will be created in order to house the tokens as a list. NOTE:THE FOLLOWING IS NO LONGER TRUE, AS IT DID NOT PROVIDE BETTER RESULTS AND IT WAS ONLY RUN ON THE `total_text` COLUMN.\n",
    "\n",
    "This will be done individually for the `title` and `text` columns, as they may hold interesting findings when trying to classify whether an article is fake or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \n",
       "ID                                                        \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...  \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...  \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...  \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...  \n",
       "875    [the, battle, of, new, york, why, this, primar...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tokens\"] = train[\"total_text\"].apply(nltk.word_tokenize, 0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train = pd.DataFrame(index=train.index)\n",
    "#tokenized_title_train[\"tokens\"] = train[\"title\"].apply(nltk.word_tokenize, 0)\n",
    "\n",
    "#tokenized_text_train = pd.DataFrame(index=train.index)\n",
    "#tokenized_text_train[\"tokens\"] = train[\"text\"].apply(nltk.word_tokenize, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "\n",
    "Having now tokenized the words in each of the documents in the dataset, we can now begin assigning the tag to each word in the dataset using a bigram tagger. Both these taggers will have unigram taggers as their backoff, which, in turn, will hve an affix tagger, that will have a regeular expresstion tagger as the final tagger. These taggers have been trained and tested on the Brown dataset, and has achieved an accuracy of 96.1% on the test dataset.\n",
    "\n",
    "Initally, the nltk pos_tag function was used, but after looking over the result, some rather obvious errors were present in the tags assigned, and therefore an ngram tagger was selected to take its place, as, overall, it appeared to be better suited for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_news_tagged = brown.tagged_sents(categories='news', tagset='universal')\n",
    "brown_news_words = brown.tagged_words(categories='news',  tagset='universal')\n",
    "brown_train = brown_news_tagged[100:]\n",
    "brown_test = brown_news_tagged[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9611992945326279"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regexp_tagger = RegexpTagger(\n",
    "     [(r'^-?[0-9]+(.[0-9]+)?$', 'NUM'),   # cardinal numbers\n",
    "      (r'(The|the|A|a|An|an)$', 'DET'),   # articles\n",
    "      (r'.*able$', 'ADJ'),                # adjectives\n",
    "      (r'.*ness$', 'NOUN'),               # nouns formed from adjectives\n",
    "      (r'.*ly$', 'ADV'),                  # adverbs\n",
    "      (r'.*s$', 'NOUN'),                  # plural nouns\n",
    "      (r'.*ing$', 'VERB'),                # gerunds\n",
    "      (r'.*ed$', 'VERB'),                 # past tense verbs\n",
    "      (r'.*', 'NOUN')                     # nouns (default)\n",
    "])\n",
    "\n",
    "affix_tagger = AffixTagger(brown_train, backoff=regexp_tagger)\n",
    "unigram_tagger = UnigramTagger(brown_train, backoff = affix_tagger)\n",
    "bigram_tagger = NgramTagger(2, brown_train, backoff = unigram_tagger)\n",
    "bigram_tagger.evaluate(brown_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_corpus(text):\n",
    "    try:\n",
    "        tags = bigram_tagger.tag(text)\n",
    "        #tags.append(bigram_tagger.tag(corpus_text))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tags\"] = train.tokens.apply(tag_corpus, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  \n",
       "ID                                                        \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...  \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...  \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...  \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...  \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train[\"tags\"] = tokenized_title_train.tokens.apply(tag_corpus, 0)\n",
    "#tokenized_title_train.head()\n",
    "\n",
    "#tokenized_text_train[\"tags\"] = tokenized_text_train.tokens.apply(tag_corpus, 0)\n",
    "#tokenized_text_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tags(tags):\n",
    "    return Counter(tag for word,tag in tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>tag_counts</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "      <td>{'PRON': 61, 'VERB': 248, 'NOUN': 446, 'DET': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "      <td>{'VERB': 85, 'DET': 57, 'ADJ': 25, 'NOUN': 150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "      <td>{'NOUN': 170, 'PRT': 19, 'VERB': 70, 'ADP': 66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "      <td>{'NOUN': 182, 'ADP': 52, 'ADJ': 28, 'DET': 44,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "      <td>{'DET': 36, 'NOUN': 112, 'ADP': 37, 'ADJ': 26,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  \\\n",
       "ID                                                         \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...   \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...   \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...   \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...   \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...   \n",
       "\n",
       "                                              tag_counts  \n",
       "ID                                                        \n",
       "8476   {'PRON': 61, 'VERB': 248, 'NOUN': 446, 'DET': ...  \n",
       "10294  {'VERB': 85, 'DET': 57, 'ADJ': 25, 'NOUN': 150...  \n",
       "3608   {'NOUN': 170, 'PRT': 19, 'VERB': 70, 'ADP': 66...  \n",
       "10142  {'NOUN': 182, 'ADP': 52, 'ADJ': 28, 'DET': 44,...  \n",
       "875    {'DET': 36, 'NOUN': 112, 'ADP': 37, 'ADJ': 26,...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"tag_counts\"] = train.tags.apply(count_tags, 0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train[\"tag_counts\"] = tokenized_title_train.tags.apply(count_tags, 0)\n",
    "\n",
    "#tokenized_text_train[\"tag_counts\"] = tokenized_text_train.tags.apply(count_tags, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the counts, have been found, converting them to columns is the next step. The will be done using function that will expand the dictionary into columns, thereby enabling us to work with those values more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the dictionary into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts_text = train['tag_counts'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  PRON   VERB   NOUN  \\\n",
       "ID                                                                             \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...  61.0  248.0  446.0   \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...  22.0   85.0  150.0   \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...  18.0   70.0  170.0   \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...   9.0   65.0  182.0   \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...  12.0   60.0  112.0   \n",
       "\n",
       "         DET    ADP   ADJ   PRT   ADV  CONJ  NUM    X    .  \n",
       "ID                                                          \n",
       "8476   188.0  164.0  83.0  49.0  75.0  47.0  5.0  NaN  NaN  \n",
       "10294   57.0   60.0  25.0  23.0  29.0  11.0  3.0  1.0  NaN  \n",
       "3608    41.0   66.0  28.0  19.0  19.0  11.0  NaN  NaN  NaN  \n",
       "10142   44.0   52.0  28.0  16.0  14.0   4.0  1.0  NaN  1.0  \n",
       "875     36.0   37.0  26.0  18.0   7.0  13.0  4.0  NaN  NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([train, tag_counts_text], axis = 1).drop('tag_counts', axis = 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  PRON   VERB   NOUN  \\\n",
       "ID                                                                             \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...  61.0  248.0  446.0   \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...  22.0   85.0  150.0   \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...  18.0   70.0  170.0   \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...   9.0   65.0  182.0   \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...  12.0   60.0  112.0   \n",
       "\n",
       "         DET    ADP   ADJ   PRT   ADV  CONJ  NUM    X    .  \n",
       "ID                                                          \n",
       "8476   188.0  164.0  83.0  49.0  75.0  47.0  5.0  0.0  0.0  \n",
       "10294   57.0   60.0  25.0  23.0  29.0  11.0  3.0  1.0  0.0  \n",
       "3608    41.0   66.0  28.0  19.0  19.0  11.0  0.0  0.0  0.0  \n",
       "10142   44.0   52.0  28.0  16.0  14.0   4.0  1.0  0.0  1.0  \n",
       "875     36.0   37.0  26.0  18.0   7.0  13.0  4.0  0.0  0.0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.fillna(0, inplace=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tag_counts_title = tokenized_title_train['tag_counts'].apply(pd.Series)\n",
    "#tag_counts_text = tokenized_text_train['tag_counts'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train = pd.concat([tokenized_title_train, tag_counts_title], axis = 1).drop('tag_counts', axis = 1)\n",
    "\n",
    "#tokenized_text_train = pd.concat([tokenized_text_train, tag_counts_text], axis = 1).drop('tag_counts', axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train.fillna(0, inplace=True)\n",
    "\n",
    "#tokenized_text_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several feature will now be created, which include: \n",
    "- length in chracters\n",
    "- number of words\n",
    "- average word lenght\n",
    "- number of sentences\n",
    "- words per sentence\n",
    "- characters per sentence\n",
    "- percentage of each part of speech of the total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_counter(df):\n",
    "    return df[\"PRON\"]+df[\"VERB\"]+df[\"NOUN\"]+df[\"DET\"]+df[\"ADJ\"]+df[\"ADP\"]+df[\"PRT\"]+df[\".\"]+df[\"ADV\"]+df[\"CONJ\"]+df[\"NUM\"]+df[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>PRT</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>NUM</th>\n",
       "      <th>X</th>\n",
       "      <th>.</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1366.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>466.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>442.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>416.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>325.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  PRON   VERB   NOUN  \\\n",
       "ID                                                                             \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...  61.0  248.0  446.0   \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...  22.0   85.0  150.0   \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...  18.0   70.0  170.0   \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...   9.0   65.0  182.0   \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...  12.0   60.0  112.0   \n",
       "\n",
       "         DET    ADP   ADJ   PRT   ADV  CONJ  NUM    X    .  word_count  \n",
       "ID                                                                      \n",
       "8476   188.0  164.0  83.0  49.0  75.0  47.0  5.0  0.0  0.0      1366.0  \n",
       "10294   57.0   60.0  25.0  23.0  29.0  11.0  3.0  1.0  0.0       466.0  \n",
       "3608    41.0   66.0  28.0  19.0  19.0  11.0  0.0  0.0  0.0       442.0  \n",
       "10142   44.0   52.0  28.0  16.0  14.0   4.0  1.0  0.0  1.0       416.0  \n",
       "875     36.0   37.0  26.0  18.0   7.0  13.0  4.0  0.0  0.0       325.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"word_count\"] = train.apply(word_counter, 1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_text_train[\"word_count\"] = tokenized_text_train.apply(word_counter, 1)\n",
    "\n",
    "#tokenized_title_train[\"word_count\"] = tokenized_title_train.apply(word_counter, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"length\"] = train['total_text'].apply(len, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"length\"] = pd.to_numeric(train[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_text_train[\"text_length\"] = tokenized_text_train['text'].apply(len, 0)\n",
    "#tokenized_title_train[\"title_length\"] = tokenized_title_train['text'].apply(len, 0)\n",
    "#tokenized_text_train[\"text_length\"] = pd.to_numeric(tokenized_text_train[\"text_length\"])\n",
    "#tokenized_title_train[\"title_length\"] = pd.to_numeric(tokenized_text_train[\"title_length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"avg_word_length\"] = train[\"length\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_title_train[\"avg_word_length\"] = tokenized_title_train[\"title_length\"] / tokenized_title_train[\"word_count\"]\n",
    "#tokenized_text_train[\"avg_word_length\"] = tokenized_text_train[\"text_length\"] / tokenized_text_train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences = train_copy.text.apply((nltk.sent_tokenize),0)\n",
    "train[\"number_of_sentences\"] = number_of_sentences.apply(len,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"number_of_sentences\"] = pd.to_numeric(train[\"number_of_sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words_per_sentence\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words_per_sentence\"] = pd.to_numeric(train[\"words_per_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words_per_sentence\"] = train[\"word_count\"] / train[\"number_of_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"words_per_sentence\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"characters_per_sentence\"] = train[\"length\"] / train[\"number_of_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train = tokenized_text_train.add_prefix('text_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tokenized_title_train = tokenized_title_train.add_prefix('title_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_comb = pd.concat([tokenized_text_train, tokenized_title_train], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"adj_per\"] = train[\"ADJ\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"verb_per\"] = train[\"VERB\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"noun_per\"] = train[\"NOUN\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"det_per\"] = train[\"DET\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"adv_per\"] = train[\"ADV\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"adp_per\"] = train[\"ADP\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"prt_per\"] = train[\"PRT\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"conj_per\"] = train[\"CONJ\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"num_per\"] = train[\"NUM\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"X_per\"] = train[\"X\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"pron_per\"] = train[\"PRON\"] / train[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>total_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tags</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>DET</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>...</th>\n",
       "      <th>verb_per</th>\n",
       "      <th>noun_per</th>\n",
       "      <th>det_per</th>\n",
       "      <th>adv_per</th>\n",
       "      <th>adp_per</th>\n",
       "      <th>prt_per</th>\n",
       "      <th>conj_per</th>\n",
       "      <th>num_per</th>\n",
       "      <th>X_per</th>\n",
       "      <th>pron_per</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8476</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>you can smell hillary’s fear daniel greenfield...</td>\n",
       "      <td>[you, can, smell, hillary, ’, s, fear, daniel,...</td>\n",
       "      <td>[(you, PRON), (can, VERB), (smell, VERB), (hil...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>446.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181552</td>\n",
       "      <td>0.326501</td>\n",
       "      <td>0.137628</td>\n",
       "      <td>0.054905</td>\n",
       "      <td>0.120059</td>\n",
       "      <td>0.035871</td>\n",
       "      <td>0.034407</td>\n",
       "      <td>0.003660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10294</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>[watch, the, exact, moment, paul, ryan, commit...</td>\n",
       "      <td>[(watch, VERB), (the, DET), (exact, ADJ), (mom...</td>\n",
       "      <td>22.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182403</td>\n",
       "      <td>0.321888</td>\n",
       "      <td>0.122318</td>\n",
       "      <td>0.062232</td>\n",
       "      <td>0.128755</td>\n",
       "      <td>0.049356</td>\n",
       "      <td>0.023605</td>\n",
       "      <td>0.006438</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.047210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>REAL</td>\n",
       "      <td>kerry to go to paris in gesture of sympathy us...</td>\n",
       "      <td>[kerry, to, go, to, paris, in, gesture, of, sy...</td>\n",
       "      <td>[(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.092760</td>\n",
       "      <td>0.042986</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>0.042986</td>\n",
       "      <td>0.024887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10142</th>\n",
       "      <td>FAKE</td>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>[bernie, supporters, on, twitter, erupt, in, a...</td>\n",
       "      <td>[(bernie, NOUN), (supporters, NOUN), (on, ADP)...</td>\n",
       "      <td>9.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.105769</td>\n",
       "      <td>0.033654</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>REAL</td>\n",
       "      <td>the battle of new york why this primary matter...</td>\n",
       "      <td>[the, battle, of, new, york, why, this, primar...</td>\n",
       "      <td>[(the, DET), (battle, NOUN), (of, ADP), (new, ...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.184615</td>\n",
       "      <td>0.344615</td>\n",
       "      <td>0.110769</td>\n",
       "      <td>0.021538</td>\n",
       "      <td>0.113846</td>\n",
       "      <td>0.055385</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.012308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                         total_text  \\\n",
       "ID                                                               \n",
       "8476   FAKE  you can smell hillary’s fear daniel greenfield...   \n",
       "10294  FAKE  watch the exact moment paul ryan committed pol...   \n",
       "3608   REAL  kerry to go to paris in gesture of sympathy us...   \n",
       "10142  FAKE  bernie supporters on twitter erupt in anger ag...   \n",
       "875    REAL  the battle of new york why this primary matter...   \n",
       "\n",
       "                                                  tokens  \\\n",
       "ID                                                         \n",
       "8476   [you, can, smell, hillary, ’, s, fear, daniel,...   \n",
       "10294  [watch, the, exact, moment, paul, ryan, commit...   \n",
       "3608   [kerry, to, go, to, paris, in, gesture, of, sy...   \n",
       "10142  [bernie, supporters, on, twitter, erupt, in, a...   \n",
       "875    [the, battle, of, new, york, why, this, primar...   \n",
       "\n",
       "                                                    tags  PRON   VERB   NOUN  \\\n",
       "ID                                                                             \n",
       "8476   [(you, PRON), (can, VERB), (smell, VERB), (hil...  61.0  248.0  446.0   \n",
       "10294  [(watch, VERB), (the, DET), (exact, ADJ), (mom...  22.0   85.0  150.0   \n",
       "3608   [(kerry, NOUN), (to, PRT), (go, VERB), (to, PR...  18.0   70.0  170.0   \n",
       "10142  [(bernie, NOUN), (supporters, NOUN), (on, ADP)...   9.0   65.0  182.0   \n",
       "875    [(the, DET), (battle, NOUN), (of, ADP), (new, ...  12.0   60.0  112.0   \n",
       "\n",
       "         DET    ADP   ADJ  ...  verb_per  noun_per   det_per   adv_per  \\\n",
       "ID                         ...                                           \n",
       "8476   188.0  164.0  83.0  ...  0.181552  0.326501  0.137628  0.054905   \n",
       "10294   57.0   60.0  25.0  ...  0.182403  0.321888  0.122318  0.062232   \n",
       "3608    41.0   66.0  28.0  ...  0.158371  0.384615  0.092760  0.042986   \n",
       "10142   44.0   52.0  28.0  ...  0.156250  0.437500  0.105769  0.033654   \n",
       "875     36.0   37.0  26.0  ...  0.184615  0.344615  0.110769  0.021538   \n",
       "\n",
       "        adp_per   prt_per  conj_per   num_per     X_per  pron_per  \n",
       "ID                                                                 \n",
       "8476   0.120059  0.035871  0.034407  0.003660  0.000000  0.044656  \n",
       "10294  0.128755  0.049356  0.023605  0.006438  0.002146  0.047210  \n",
       "3608   0.149321  0.042986  0.024887  0.000000  0.000000  0.040724  \n",
       "10142  0.125000  0.038462  0.009615  0.002404  0.000000  0.021635  \n",
       "875    0.113846  0.055385  0.040000  0.012308  0.000000  0.036923  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmas for each of the words will now be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lemma\"] = train[\"tokens\"].apply(lemmatize, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diffferent Vectorization techniques will be tried. The one that provides the best results will be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_weighting = TfidfVectorizer(stop_words = \"english\")\n",
    "tf_idf_train = tf_idf_weighting.fit_transform(train.total_text)\n",
    "tf_idf = pd.DataFrame(tf_idf_train.A, columns=tf_idf_weighting.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>'</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>...</th>\n",
       "      <th>’ t ’</th>\n",
       "      <th>’ v</th>\n",
       "      <th>’ v e</th>\n",
       "      <th>’ w</th>\n",
       "      <th>’ w e</th>\n",
       "      <th>’ w h</th>\n",
       "      <th>’ y</th>\n",
       "      <th>’ ’</th>\n",
       "      <th>’ ’</th>\n",
       "      <th>’ ’ s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.633118</td>\n",
       "      <td>0.019923</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001417</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.629535</td>\n",
       "      <td>0.035121</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003997</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002036</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.613432</td>\n",
       "      <td>0.016036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002074</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.588454</td>\n",
       "      <td>0.049027</td>\n",
       "      <td>0.013224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.003979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.602517</td>\n",
       "      <td>0.017633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00345</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 11959 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     '         b         c         d  \\\n",
       "0  0.633118  0.019923  0.000761    0.0  0.001417  0.001284  0.001415   \n",
       "1  0.629535  0.035121  0.002147    0.0  0.003997  0.001811  0.000000   \n",
       "2  0.613432  0.016036  0.000000    0.0  0.002074  0.001880  0.002071   \n",
       "3  0.588454  0.049027  0.013224    0.0  0.000000  0.001859  0.000000   \n",
       "4  0.602517  0.017633  0.000000    0.0  0.000000  0.007578  0.000000   \n",
       "\n",
       "          e         f        g  ...  ’ t ’       ’ v     ’ v e  ’ w  ’ w e  \\\n",
       "0  0.000000  0.001444  0.00000  ...    0.0  0.000000  0.000000  0.0    0.0   \n",
       "1  0.000000  0.002036  0.00000  ...    0.0  0.000000  0.000000  0.0    0.0   \n",
       "2  0.000000  0.000000  0.00000  ...    0.0  0.000000  0.000000  0.0    0.0   \n",
       "3  0.003483  0.002090  0.00000  ...    0.0  0.003977  0.003979  0.0    0.0   \n",
       "4  0.000000  0.000000  0.00345  ...    0.0  0.000000  0.000000  0.0    0.0   \n",
       "\n",
       "   ’ w h  ’ y  ’ ’  ’ ’    ’ ’ s  \n",
       "0    0.0  0.0  0.0    0.0    0.0  \n",
       "1    0.0  0.0  0.0    0.0    0.0  \n",
       "2    0.0  0.0  0.0    0.0    0.0  \n",
       "3    0.0  0.0  0.0    0.0    0.0  \n",
       "4    0.0  0.0  0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 11959 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_weighting = TfidfVectorizer(stop_words = \"english\", ngram_range=(1,3), tokenizer=lemmatize, min_df=2)\n",
    "ngram_train = ngrams_weighting.fit_transform(train.total_text)\n",
    "ngram = pd.DataFrame(ngram_train.A, columns=ngrams_weighting.get_feature_names())\n",
    "ngram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram = ngram.set_index(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3999, 11959)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will find code that was used in the previous attempts when the title and text were treated as separate entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#tf_idf_weighting = TfidfVectorizer()\n",
    "#tf_idf_news_train = tf_idf_weighting.fit_transform(train.text)\n",
    "#tf_idf_text = pd.DataFrame(tf_idf_news_train.A, columns=tf_idf_weighting.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_text = tf_idf_text.add_prefix(\"text_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_weighting = TfidfVectorizer()\n",
    "#tf_idf_title_train = tf_idf_weighting.fit_transform(train.title)\n",
    "#tf_idf_title = pd.DataFrame(tf_idf_title_train.A, columns=tf_idf_weighting.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_title = tf_idf_title.set_index(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_title = tf_idf_title.add_prefix(\"title_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_idf_comb = pd.concat([tf_idf_text, tf_idf_title], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine the best vectorizer with the features that have been created into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = pd.concat([train, ngram], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph we can note that the sample size is realtively equal for both the real and fake news article, hence bagging or bootstrapping won't be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_1 = train.groupby(\"label\")\n",
    "label_1.size().plot.barh(color=['green','navy'])\n",
    "pyplot.xlabel(\"Label\")\n",
    "pyplot.ylabel(\"Number\")\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'POS Taggers Distribution')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.figure(figsize=(15, 10))\n",
    "bins = np.linspace(0, 1000, 100)\n",
    "\n",
    "PRON = train[\"PRON\"]\n",
    "VERB = train[\"VERB\"]\n",
    "NOUN = train[\"NOUN\"]\n",
    "DET = train[\"DET\"]\n",
    "ADJ = train[\"ADJ\"]\n",
    "pyplot.hist(PRON, bins, alpha=0.5, label='PRON')\n",
    "pyplot.hist(VERB, bins, alpha=0.5, label='VERB')\n",
    "pyplot.hist(NOUN, bins, alpha=0.5, label='NOUN')\n",
    "pyplot.hist(DET, bins, alpha=0.5, label='DET')\n",
    "pyplot.hist(ADJ, bins, alpha=0.5, label='ADJ')\n",
    "pyplot.title(\"POS Taggers Distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the features that will not be inserted into a model will be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all = train_all.drop([\"label\", \"tags\", \"tokens\", \"total_text\", \"lemma\", \"PRON\", \"VERB\", \"NOUN\", \"DET\", \"ADJ\", \"ADP\", \"PRT\", \".\", \"ADV\", \"CONJ\" , \"NUM\", \"X\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Splitting\n",
    "\n",
    "Now that the data has been prepared, the train data will be split into a training set and a testing set to see the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(train_all, labels, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above combination of both sets results in lower performing models, or the models did not run. As a result, only the results from the vectorizers were inputted into the models, which is shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1,x_test_1,y_train_1,y_test_1=train_test_split(ngram, labels, test_size=0.2, random_state=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(x_train_1, y_train_1)\n",
    "pred = naive_bayes.predict(x_test_1)\n",
    "cv = cross_val_score(naive_bayes, x_train_1, y_train_1, scoring = 'accuracy', cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8709032668231611"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the Naive Bayes was not optimal, as the inital target was an accuracy of >90%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Passive-Agressive Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next classifier to be tested is the Passive-Aggreive classifer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    7.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.62%\n"
     ]
    }
   ],
   "source": [
    "pac_parameters = [{'max_iter': [50], \n",
    "                   'C': [30],\n",
    "                   'random_state': [22]}\n",
    "                ]\n",
    "\n",
    "pac = GridSearchCV(\n",
    "        PassiveAggressiveClassifier(), pac_parameters, scoring='accuracy', verbose = 10, n_jobs = -1\n",
    "    )\n",
    "pac.fit(x_train_1,y_train_1)\n",
    "#DataFlair - Predict on the test set and calculate accuracy\n",
    "y_pred=pac.predict(x_test_1)\n",
    "score=metrics.accuracy_score(y_test_1,y_pred)\n",
    "print(f'Accuracy: {round(score*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 30, 'max_iter': 50, 'random_state': 22}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pac.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here the performance of the passive aggresive classifier was below the required threshold. The parameters input into this model, were those that gave the best result after a grid search. To save time, only the best parameters were kept, incase the notebook has to be rerun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                           decision_function_shape='ovr', degree=3,\n",
       "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                           probability=False, random_state=None, shrinking=True,\n",
       "                           tol=0.001, verbose=False),\n",
       "             iid='warn', n_jobs=-1,\n",
       "             param_grid=[{'C': [100], 'gamma': [1e-07], 'kernel': ['linear'],\n",
       "                          'random_state': [22]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=10)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_parameters = [{'kernel': ['linear'], 'gamma': [1e-7],\n",
    "                     'C': [100],\n",
    "                     'random_state': [22]}\n",
    "                    ]\n",
    "\n",
    "svm = GridSearchCV(\n",
    "        SVC(), svm_parameters, scoring='accuracy', verbose = 10, n_jobs = -1\n",
    "    )\n",
    "\n",
    "svm.fit(x_train_1, y_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=svm.predict(x_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91875"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test_1,y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 100, 'gamma': 1e-07, 'kernel': 'linear', 'random_state': 22}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the SVM model was able to give a model the exceeds the minimum score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    6.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:    6.2s finished\n",
      "C:\\Users\\ignac\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "logistic_parameters = [\n",
    "    {'C': [10000],\n",
    "    \"random_state\": [22]}\n",
    "                    ]\n",
    "\n",
    "\n",
    "logistic_model = GridSearchCV(\n",
    "       LogisticRegression(), logistic_parameters, scoring='accuracy', n_jobs=-1, verbose=10\n",
    "    )\n",
    "\n",
    "logistic_model.fit(x_train_1, y_train_1)\n",
    "\n",
    "y_pred = logistic_model.predict(x_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10000, 'random_state': 22}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9275"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test_1,y_pred)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the logistic model performed better than the SVM, therefore it will be the final model of choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below steps are the same transformations that were conducted on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = news_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"text\"] = test[\"text\"].str.lower()\n",
    "test[\"title\"] = test[\"title\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"title\"] = test[\"title\"].apply(regex, 0)\n",
    "test[\"text\"] = test[\"text\"].apply(regex, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>total_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>september new homes sales riseback to  level</td>\n",
       "      <td>september new homes sales rise back to  level ...</td>\n",
       "      <td>september new homes sales riseback to  level s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>why the obamacare doomsday cult can't admit it...</td>\n",
       "      <td>but when congress debated and passed the patie...</td>\n",
       "      <td>why the obamacare doomsday cult can't admit it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>sanders cruz resist pressure after ny losses v...</td>\n",
       "      <td>the bernie sanders and ted cruz campaigns vowe...</td>\n",
       "      <td>sanders cruz resist pressure after ny losses v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4128</th>\n",
       "      <td>surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>police searching for the second of two escaped...</td>\n",
       "      <td>surviving escaped prisoner likely fatigued and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>clinton and sanders neck and neck in californi...</td>\n",
       "      <td>no matter who wins california's  delegates on ...</td>\n",
       "      <td>clinton and sanders neck and neck in californi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "ID                                                         \n",
       "10498       september new homes sales riseback to  level   \n",
       "2439   why the obamacare doomsday cult can't admit it...   \n",
       "864    sanders cruz resist pressure after ny losses v...   \n",
       "4128   surviving escaped prisoner likely fatigued and...   \n",
       "662    clinton and sanders neck and neck in californi...   \n",
       "\n",
       "                                                    text  \\\n",
       "ID                                                         \n",
       "10498  september new homes sales rise back to  level ...   \n",
       "2439   but when congress debated and passed the patie...   \n",
       "864    the bernie sanders and ted cruz campaigns vowe...   \n",
       "4128   police searching for the second of two escaped...   \n",
       "662    no matter who wins california's  delegates on ...   \n",
       "\n",
       "                                              total_text  \n",
       "ID                                                        \n",
       "10498  september new homes sales riseback to  level s...  \n",
       "2439   why the obamacare doomsday cult can't admit it...  \n",
       "864    sanders cruz resist pressure after ny losses v...  \n",
       "4128   surviving escaped prisoner likely fatigued and...  \n",
       "662    clinton and sanders neck and neck in californi...  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"total_text\"] = test[\"title\"] + \" \" + test[\"text\"]\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop([\"title\", \"text\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"tokens\"] = test[\"total_text\"].apply(nltk.word_tokenize, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"tags\"] = test.tokens.apply(tag_corpus, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"tag_counts\"] = test.tags.apply(count_tags, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_counts_test = test['tag_counts'].apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test, tag_counts_test], axis = 1).drop('tag_counts', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"word_count\"] = test.apply(word_counter, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"length\"] = test['total_text'].apply(len, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"length\"] = pd.to_numeric(test[\"length\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"avg_word_length\"] = test[\"length\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_sentences_test = news_test.text.apply((nltk.sent_tokenize),0)\n",
    "test[\"number_of_sentences\"] = number_of_sentences_test.apply(len,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"number_of_sentences\"] = pd.to_numeric(test[\"number_of_sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"words_per_sentence\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"words_per_sentence\"] = pd.to_numeric(test[\"words_per_sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"words_per_sentence\"] = test[\"word_count\"] / test[\"number_of_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"words_per_sentence\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"characters_per_sentence\"] = test[\"length\"] / test[\"number_of_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"adj_per\"] = test[\"ADJ\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"verb_per\"] = test[\"VERB\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"noun_per\"] = test[\"NOUN\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"det_per\"] = test[\"DET\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"adv_per\"] = test[\"ADV\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"adp_per\"] = test[\"ADP\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"prt_per\"] = test[\"PRT\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"conj_per\"] = test[\"CONJ\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"num_per\"] = test[\"NUM\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"X_per\"] = test[\"X\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"pron_per\"] = test[\"PRON\"] / test[\"word_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"lemma\"] = test[\"tokens\"].apply(lemmatize, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_test = ngrams_weighting.transform(test.total_text)\n",
    "ngram_t = pd.DataFrame(ngram_test.A, columns=ngrams_weighting.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_t = ngram_t.set_index(test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = pd.concat([test, ngram_t], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = ngram_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the logistic model that was found to perform best before, the following are the predicitions for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = logistic_model.predict(ngram_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS = pd.DataFrame(final_predictions, columns=[\"label\"], index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "count   2321\n",
       "unique     2\n",
       "top     REAL\n",
       "freq    1169"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PREDICTIONS.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS.to_csv(\"NLP_assignment_1_submission\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
